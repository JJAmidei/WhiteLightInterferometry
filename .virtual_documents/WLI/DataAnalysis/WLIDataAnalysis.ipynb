import numpy as np
import matplotlib.pyplot as plt
import os
import scipy.io as sio
from scipy.signal import correlate
import h5py
from scipy.stats import pearsonr
import matplotlib.pyplot as plt


# Plot style parameters
plt.rcParams['font.family'] = 'Arial'
plt.rc('font', size=16)          # text size
plt.rc('axes', titlesize=20)     # axes title size
plt.rc('axes', labelsize=20)     # axes label size
plt.rc('xtick', labelsize=16)    # x-tick label size
plt.rc('ytick', labelsize=16)    # y-tick label size
plt.rc('legend', fontsize=18)    # legend font size
plt.rc('figure', titlesize=16)   # figure title size


#%ls ../Team32025/WLI_With_Glass_20251015/ # list data folder/files
#%ls ..\Team32025\


def extract_mat_data(folder):
    '''
    This is the first function I tried to pull the data out of the .mat files.
    It generally works fine, but requires you to pull the data by first calling
    the function with a file path as input. It then looks in that file and extracts
    the data from each file that ends with the .mat extension. This is called as follows:
    > path = '/this/is/the/path/to/the/data/and/is/a/string'
    > data = extract_mat_data(path)   
    To then pull the extracted data, you use the filename as a key. You can run 
    > data.keys()
    to see which keys are availible. As said above, these are going to be the filenames
    of the .mat file from which the data were extracted. They are strings, so use '' or "". 
    To then look at the way the data is structured, you can take one of these keys 
    and run the following:
    > data['key'].items()
    This gives you a general idea as to the structure of the data inside of each .mat file. 
    To then look at a specfic piece of data, like if you wanted to plot it for instance, 
    it starts to get a bit convoluted. You have to add the item name onto the end like so:
    > data['key']['item']
    As an example, with the 'raw_data' files in the WLI data we took, you will get an item 
    called 'dataArray' which is [2048,2] array, where each column is a different set of data 
    from the HeNe source and the White Light source respectively. So in order to plot one 
    of those datasets, you would need to grab each column separately. This is done like so
    (assuming matplotlib.pyplot has been imported as plt):
    > HeNeData = data['raw_data_file']['dataArray'][:,0] # first col: HeNe source
    > WLData = data['raw_data_file']['dataArray'][:,1] # second col: White Light source
    > fig, ax = plt.subplots(1,1, figsize=(8,6))
    > ax.plot(HeNeData, label='HeNe')
    > ax.plot(WLData, label='White Light')
    > ax.legend()
    > plt.show()
    As you can see, calling the data the way it is extracted by this function quickly becomes 
    cumbersome and inefficient. So I'd recommend not using this function. I do have it saved 
    just in case it proves useful for some other reason.
    '''
    data_dict = {}

    for filename in os.listdir(folder):
        if filename.endswith('.mat'):
            file_path = os.path.join(folder, filename)
            file_key = os.path.splitext(filename)[0]

            try:
                # Load scipy.io (for most MATLAB files)
                mat_data = sio.loadmat(file_path)
                
                # Filter out MATLAB metadata keys that start with '__'
                clean_data = {k: v for k, v in mat_data.items() if not k.startswith('__')}
                
                data_dict[file_key] = clean_data

            except NotImplementedError:
                # If loadmat fails, it's likely a v7.3 (HDF5-based) file
                with h5py.File(file_path, 'r') as f:
                    h5_data = {}
                    def recursively_load(name, obj):
                        if isinstance(obj, h5py.Dataset):
                            h5_data[name] = np.array(obj)
                    f.visititems(recursively_load)
                    data_dict[file_key] = h5_data

            except Exception as e:
                print(f"Error reading {filename}: {e}")

    return data_dict


# Makes use of extract_mat_data() function. Can ignore. 

#path = '/home/james/Desktop/WLI/Team32025/WLI_With_Glass_20251015/'
#path = '../Team32025/WLI_With_Glass_20251015/'
#data = extract_mat_data(path)

#print(data.keys())

## Inspect files
#for key in data.keys():
#    print(f"File: {key}")
#    for var_name, var_data in data[key].items():
#        print(f"  {var_name}: shape {var_data.shape}, dtype {var_data.dtype}")

# In case you wanted to see how you call a specfic piece of data
# This calls the HeNe source data from 'raw_data_004_20251015'. 
#data['raw_data_004_20251015']['dataArray'][:,0]


def extract_data(folder):
    '''
    This is a better data extraction function to use for this project. 
    In particular this extracts the dataArray columns from each file 
    and assigns them to variable names; which are the filename appended 
    with either '_HN_data' or '_WL_data' respectively. This is just for
    easier plotting through a loop since there two time the number of files
    of data that need to be plotted before averaging.
    '''
    data_dict = {}

    for filename in os.listdir(folder):
        if filename.endswith('.mat'):
            file_path = os.path.join(folder, filename)
            file_key = os.path.splitext(filename)[0]

            try:
                # Try loading with scipy.io (for most MATLAB files)
                mat_data = sio.loadmat(file_path)
                clean_data = {k: v for k, v in mat_data.items() if not k.startswith('__')}

            except NotImplementedError:
                # If it's a v7.3 (HDF5-based) file, use h5py
                with h5py.File(file_path, 'r') as f:
                    clean_data = {}
                    def recursively_load(name, obj):
                        if isinstance(obj, h5py.Dataset):
                            clean_data[name] = np.array(obj)
                    f.visititems(recursively_load)

            # Extract 'dataArray' key from file structure
            if 'dataArray' in clean_data:
                arr = clean_data['dataArray']
                if arr.ndim == 2 and arr.shape[1] >= 2:
                    HN_data = arr[:, 0]
                    WL_data = arr[:, 1]

                    data_dict[f'{file_key}_HN_data'] = HN_data
                    data_dict[f'{file_key}_WL_data'] = WL_data

            # Metadata stored here incase it is needed here later for some reason
            meta = {}
            for key in ['runTime', 'xCenCorrection', 'xLinear', 'xLinearStage']:
                if key in clean_data:
                    meta[key] = clean_data[key]
            data_dict[f'{file_key}_meta'] = meta

    return data_dict


# Specific path on my office computer. Can ignore
#path = '/home/james/Desktop/WLI/Team32025/WLI_With_Glass_20251015/'

# This path should work generally on your computer, 
# given you downloaded the full repository.
path = '../Team32025/WLI_With_Glass_20251015/'
data = extract_data(path)

# Below is just messing around and making sure seemed copacetic. 
# If you want to mess around with this, just uncomment anything below. 
# It is a bit disorganized, so you might have to deliberate about 
# what you are trying to look at. 

#print(data.keys())
#data2plot = []
#for key in data.keys():
#    if key.endswith('_HN_data') or key.endswith('_WL_data'):
#        data2plot.append(key)

# Word of warning, this will make 50 plots. That being said, it is fast.
# This plots the HeNe and WL data on separate plots, which is useful for
# a prelimary look, but not the final analysis. 
#for dataname in data2plot:
#    plt.figure(figsize=(10, 5))
#    plt.plot(data[dataname], color='k', alpha=0.8)
#    plt.xlabel('Distance [bins]')
#    plt.ylabel('Intensity [units]')
#    plt.title(dataname,  fontweight='bold')
#    plt.grid(True)
#    plt.tick_params(axis='both', which='both', direction='in', top=True, right=True)
#    plt.show()
    #print(data[dataname])

# Just looking at one files raw data
#data1HN = data['raw_data_007_20251015_HN_data']
#data1WL = data['raw_data_007_20251015_WL_data']
#metadata = data['raw_data_007_20251015_meta']
#print(metadata.keys())  # -> dict_keys(['runTime', 'xCenCorrection', ...])

#fig, ax = plt.subplots(1,1, figsize=(10,8))

#ax.plot(data1HN, color='k', alpha=0.8) # first column is HeNe
#ax.plot(data1WL, color='r', alpha=0.8) # second column is WL
#ax.set_xlabel('Distance [a.u]', fontweight='bold')
#ax.set_ylabel('Intensity [units]',  fontweight='bold')
#ax.grid(True)
#plt.tick_params(axis='both', which='both', direction='in', top=True, right=True)
#plt.show()





# This is stupid and should really probably be part of the extract_data() function.
# Basically, we need to take the base_key (key without the variable extension) 
# and reapply an extension to it, otherwise we will get duplicates because each file
# key has two variables that correspond to it. 

#start_idx = 700
#end_idx = 1550
#print(end_idx)
#for key in sorted(data.keys()):
#    if key.endswith('_HN_data'):
#        base_key = key[:-8]
#        wl_key = f'{base_key}_WL_data'
#        #print(key)
#        if wl_key in data:
#           plt.figure(figsize=(12,8))
#            plt.plot(data[key][start_idx:end_idx], label='HeNe', color='red', alpha=0.8)
#            plt.plot(data[wl_key][start_idx:end_idx], label='White Light', color='black', alpha=0.8)
#            plt.xlabel('Distance [bins]', fontweight='bold')
#            plt.ylabel('Intensity [units]', fontweight='bold')
#            plt.title(base_key, fontweight='bold')
#            plt.legend()
#            plt.grid(True)
#            plt.tick_params(axis='both', which='both', direction='in', top=True, right=True)
#            plt.show()





# Look at metadata for a few files
for i, key in enumerate(sorted(data.keys())):
    if key.endswith('_meta'):
        print(f"\n{key}:")
        meta = data[key]
        for meta_key, meta_value in meta.items():
            print(f"  {meta_key}: {meta_value}")
        if i >= 2:  # first 3 files
            break

# Thank holy fuck! Same xCenCorrection, xLinear, and xLinearStage. 
# So averaging should be relatively simple.


# Take all HN_data arrays to see if they have different baselines

hn_arrays = []
wl_arrays = []
labels = []

for key in sorted(data.keys()):
    if key.endswith('_HN_data'):
        base_key = key[:-8]
        wl_key = f'{base_key}_WL_data'
        
        if wl_key in data:
            hn_arrays.append(data[key])
            wl_arrays.append(data[wl_key])
            labels.append(base_key)

# Overlay all HN data on one plot to see variations
plt.figure(figsize=(12, 6))
for i, (hn, label) in enumerate(zip(hn_arrays, labels)):
    plt.plot(hn, alpha=0.5, label=label if i < 5 else None)  # Only label first 5 to avoid clutter
plt.xlabel('Distance [bins]', fontweight='bold')
plt.ylabel('Intensity [units]', fontweight='bold')
plt.title('All HeNe Data Overlaid', fontweight='bold')
if len(labels) <= 5:
    plt.legend()
plt.grid(True)
plt.show()

# WL data overlaid
plt.figure(figsize=(12, 6))
for i, (wl, label) in enumerate(zip(wl_arrays, labels)):
    plt.plot(wl, alpha=0.5, label=label if i < 5 else None)
plt.xlabel('Distance [bins]', fontweight='bold')
plt.ylabel('Intensity [units]', fontweight='bold')
plt.title('All White Light Overlaid', fontweight='bold')
if len(labels) <= 5:
    plt.legend()
plt.grid(True)
plt.show()


# Convert to numpy arrays
hn_arrays = np.array(hn_arrays)
wl_arrays = np.array(wl_arrays)

# Take average of all data
hn_avg = np.mean(hn_arrays, axis=0)
wl_avg = np.mean(wl_arrays, axis=0)

# Compute standard deviation to see uncertainty
hn_std = np.std(hn_arrays, axis=0)
wl_std = np.std(wl_arrays, axis=0)

# Plot averaged data without std error
plt.figure(figsize=(12, 8))
plt.plot(hn_avg, label='HeNe (averaged)', color='red', alpha=0.8, linewidth=2)
plt.plot(wl_avg, label='White Light (averaged)', color='black', alpha=0.8, linewidth=2)
plt.xlabel('Distance [bins]', fontweight='bold')
plt.ylabel('Intensity [units]', fontweight='bold')
plt.title('Averaged Data (w/o Uncertainty)', fontweight='bold')
plt.legend()
plt.grid(True)
plt.tick_params(axis='both', which='both', direction='in', top=True, right=True)
plt.show()


# Plot averaged data with std error
plt.figure(figsize=(12, 8))
plt.plot(hn_avg, label='HeNe (averaged)', color='red', alpha=0.8, linewidth=2)
plt.fill_between(range(len(hn_avg)), hn_avg - hn_std, hn_avg + hn_std, 
                 color='red', alpha=0.2, label=r'HeNe $\pm 1 \sigma$')
plt.plot(wl_avg, label='White Light (averaged)', color='black', alpha=0.8, linewidth=2)
plt.fill_between(range(len(wl_avg)), wl_avg - wl_std, wl_avg + wl_std, 
                 color='gray', alpha=0.2, label=r'WL $\pm 1 \sigma$')
plt.xlabel('Distance [bins]', fontweight='bold')
plt.ylabel('Intensity [units]', fontweight='bold')
plt.title('Averaged Data (w/ Uncertainty)', fontweight='bold')
plt.legend()
plt.grid(True)
plt.tick_params(axis='both', which='both', direction='in', top=True, right=True)
plt.show()


# Individual plots so data can be seen more clearly
plt.figure(figsize=(12, 8))
plt.plot(hn_avg, label='HeNe (averaged)', color='red', alpha=0.8, linewidth=2)
plt.fill_between(range(len(hn_avg)), hn_avg - hn_std, hn_avg + hn_std, 
                 color='red', alpha=0.2, label=r'HeNe $\pm 1 \sigma$')
plt.xlabel('Distance [bins]', fontweight='bold')
plt.ylabel('Intensity [units]', fontweight='bold')
plt.title('Averaged HeNe Data (w/ Uncertainty)', fontweight='bold')
plt.legend()
plt.grid(True)
plt.tick_params(axis='both', which='both', direction='in', top=True, right=True)
plt.show()

plt.figure(figsize=(12, 8))
plt.plot(wl_avg, label='White Light (averaged)', color='black', alpha=0.8, linewidth=2)
plt.fill_between(range(len(wl_avg)), wl_avg - wl_std, wl_avg + wl_std, 
                 color='gray', alpha=0.2, label=r'WL $\pm 1 \sigma$')
plt.xlabel('Distance [bins]', fontweight='bold')
plt.ylabel('Intensity [units]', fontweight='bold')
plt.title('Averaged White Light Data (w/ Uncertainty)', fontweight='bold')
plt.legend()
plt.grid(True)
plt.tick_params(axis='both', which='both', direction='in', top=True, right=True)
plt.show()





#fig, axes = plt.subplots(5, 1, figsize=(12, 15))
#
#for i in range(min(5, len(hn_arrays))):
#    axes[i].plot(hn_arrays[i], label=f'Scan {i}', alpha=0.8)
#    axes[i].set_xlabel('Distance [bins]', fontweight='bold')
#    axes[i].set_ylabel('Intensity', fontweight='bold')
#    axes[i].set_title(f'Scan {i}', fontweight='bold')
#    axes[i].grid(True)
#    axes[i].legend()
#
#plt.tight_layout()
#plt.show()
#
#print(f"\nIntensity ranges for first 5 scans:")
#for i in range(min(5, len(hn_arrays))):
#    print(f"Scan {i}: min={np.min(hn_arrays[i]):.2f}, max={np.max(hn_arrays[i]):.2f}, mean={np.mean(hn_arrays[i]):.2f}")


start_bin = 0
end_bin = 2047


hn_cropped = [hn[start_bin:end_bin] for hn in hn_arrays]
wl_cropped = [wl[start_bin:end_bin] for wl in wl_arrays]

n_scans = len(hn_cropped)
corr_matrix = np.zeros((n_scans, n_scans))

for i in range(n_scans):
    for j in range(n_scans):
        #print(f'i: {i}, j: {j}')
        corr, _ = pearsonr(hn_cropped[i], hn_cropped[j])
        corr_matrix[i, j] = corr

# Plot correlation matrix
plt.figure(figsize=(10, 8))
im = plt.imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1)
plt.colorbar(im, label='Correlation')
plt.xlabel('Scan number', fontweight='bold')
plt.ylabel('Scan number', fontweight='bold')
plt.title('Pairwise Correlation Matrix', fontweight='bold')
plt.tight_layout()
plt.show()

fig, axes = plt.subplots(3, 1, figsize=(14, 10))

best_match_idx = np.argmax(corr_matrix[0, 1:]) + 1
axes[0].plot(hn_cropped[0], label='Scan 0', alpha=0.7)
axes[0].plot(hn_cropped[best_match_idx], label=f'Scan {best_match_idx} (best corr={corr_matrix[0, best_match_idx]:.3f})', alpha=0.7)
axes[0].set_title('Best correlation pair', fontweight='bold')
axes[0].legend()
axes[0].grid(True)

worst_match_idx = np.argmin(corr_matrix[0, 1:]) + 1
axes[1].plot(hn_cropped[0], label='Scan 0', alpha=0.7)
axes[1].plot(hn_cropped[worst_match_idx], label=f'Scan {worst_match_idx} (worst corr={corr_matrix[0, worst_match_idx]:.3f})', alpha=0.7)
axes[1].set_title('Worst correlation pair', fontweight='bold')
axes[1].legend()
axes[1].grid(True)

zoom_start = 100
zoom_end = 200
axes[2].plot(hn_cropped[0][zoom_start:zoom_end], 'o-', label='Scan 0', alpha=0.7)
axes[2].plot(hn_cropped[worst_match_idx][zoom_start:zoom_end], 'o-', label=f'Scan {worst_match_idx}', alpha=0.7)
axes[2].set_title('Zoomed view - checking for phase shifts', fontweight='bold')
axes[2].set_xlabel('Bin (relative)', fontweight='bold')
axes[2].legend()
axes[2].grid(True)

plt.tight_layout()
plt.show()


scan_num=24
start=0
end=600
scan_shifted = np.roll(wl_cropped[scan_num], -10)
correlation, _ = pearsonr(wl_cropped[0], scan_shifted)
fig, axes = plt.subplots(2,1, figsize=(20,10))
axes[0].plot(wl_cropped[0][start:end], '-', color='blue', label='Scan 0')
axes[0].plot(wl_cropped[scan_num][start:end], '-', color='green', label=f'Scan {scan_num}')
axes[0].legend(loc='lower right')
axes[1].plot(wl_cropped[0][start:end], '-', color='blue', label='Scan 0')
axes[1].plot(scan_shifted[start:end], '-', color='green', label=f'Scan {scan_num} shifted')
plt.show()
print(correlation)

# 0 and 24 -  4 bins
# 0 and 23 -  0 bins
# ..... 22 -  3 bins
# ..... 21 - -4 bins
# ..... 20 - 3 bins
# ..... 19 - 3 bins
# ..... 18 - 3 bins
# ..... 17 - 3 bins
# ..... 16 - 3 bins
# ..... 15 - 3 bins
# ..... 14 - 3 bins
# ..... 13 - 3 bins
# ..... 12 - 3 bins
# ..... 11 - 3 bins
# ..... 10 - 3 bins
# .....  9 - 3 bins
# .....  8 - 3 bins
# .....  7 - 3 bins
# .....  6 - 3 bins
# .....  5 - 3 bins
# .....  4 - 3 bins
# .....  3 - 3 bins
# .....  2 - 3 bins
# .....  1 - 3 bins
# .....  0 - 3 bins


n = len(hn_cropped)
for i in range(n):
    corr, _ = pearsonr(wl_cropped[0], wl_cropped[i])    
    print(f'Correlation between Scan 0 and Scan {i}: {corr}')


max_shift = 12  # maximum shift
threshold = 0.1  # min correlation

n = len(hn_cropped)
hn_shifted = np.zeros(n)
wl_shifted = np.zeros(n)
reference = wl_cropped[0]

for i in range(1, n):  # skips reference scan 
    best_corr = -1
    best_shift = 0
    
    # try shifts from -max_shift to +max_shift
    for shift in range(-max_shift, max_shift + 1):
        shifted = np.roll(wl_cropped[i], shift)
        corr, _ = pearsonr(reference, shifted)
        if corr > best_corr:
            best_corr = corr
            best_shift = shift

    # Apply the best shift if correlation is below threshold
    if best_corr < threshold:
        print(f"Scan {i}: best corr = {best_corr:.3f}, but below threshold ({threshold}). Applying shift {best_shift}.")
    else:
        print(f"Scan {i}: best corr = {best_corr:.3f}, applying shift {best_shift}.")
    
    # Shift the data
    hn_cropped[i] = np.roll(hn_cropped[i], best_shift)
    wl_cropped[i] = np.roll(wl_cropped[i], best_shift)


plt.plot( wl_cropped[0])



